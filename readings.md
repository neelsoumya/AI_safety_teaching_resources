Threat Models Readings, Timelines Assignments & Further Readings

Thanks for attending the first workshop of our AI Safety & Strategy Fast-Track!
Looking forward to seeing you at the upcoming second compulsory intro workshops, where we will discuss potential emergent risks from advanced AI systems.

Before attending, we ask you to:
Read the preliminary readings related to the Threat Models Workshop
Complete & submit your assignments relating to the Timelines Workshop
(Optionally) read our suggested additional reading on timelines

We ask you to complete your assignments in a Google doc, and submit it via this form.
Threat Models Workshop Preliminary Readings
A broad overview of risks from advanced AI
https://aisafetyfundamentals.com/blog/ai-risks/

Bird’s eye view essays about loss of control
https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like
https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/

Scheming
AIs “scheming” and subverting our evaluations or the training process itself is a key consideration in catastrophic loss of control scenarios.
At a high level, why could AIs “want” to deceive us or seek power?
https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/

But how likely is this in practice?
Recent research by Anthropic aims to investigate this question empirically in “model organisms” setups.
(Required reading is only the two blogposts, but if interested, encouraged to check out the full paper.)

https://www.anthropic.com/research/alignment-faking
https://www.anthropic.com/research/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training




AI Timelines Workshop – Assignments
Write down 5 specific predictions with regards to AI capabilities & frontier results in the next year. Be as specific as possible!
Do you think AI will be able to significantly (10x) accelerate Machine Learning research itself end-to-end eventually? If yes, try to put a tentative estimate on the time you think this is reasonable to expect.
What do you think could prevent AI capabilities from scaling to this level?
Even if AIs are “smart enough”, there might be different effects dampening R&D speedup. What in your opinion are the most likely relevant factors?
What early warning signs in your opinion would indicate we are nearing AI that would be able to act as a competent ML researcher? How would we be better able to forecast this?


Further recommended readings on AI timelines
If you wanted to think more about when AI systems capable of speeding up R&D significantly might arrive, we think the following posts have some quite good arguments for shorter timelines:
https://www.lesswrong.com/posts/K4urTDkBbtNuLivJx/why-i-think-strong-general-ai-is-coming-soon


Lastly, if you are interested in following closely the unfolding AI developments & how leading figures view it, we can highly recommend Dwarkesh Patel’s podcast, where he interviews AI researchers, leading industry CEOs as well as prolific figures in history, economics and politics. This is not a sponsorship, we are just a fan.
